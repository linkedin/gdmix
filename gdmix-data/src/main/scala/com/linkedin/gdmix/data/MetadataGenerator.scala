package com.linkedin.gdmix.data

import scala.collection.mutable

import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.hadoop.mapred.JobConf
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions.max
import org.apache.spark.sql.types.{ArrayType, NumericType, StructField, StructType}
import org.json4s.DefaultFormats
import org.json4s.ext.EnumNameSerializer
import org.json4s.jackson.JsonMethods.parse

import com.linkedin.gdmix.configs.{DataType, DatasetMetadata, TensorMetadata}
import com.linkedin.gdmix.utils.Constants._
import com.linkedin.gdmix.utils.ConversionUtils.mapSparkToConfigDataType
import com.linkedin.gdmix.utils.{IoUtils, JsonUtils}

/**
 * Generate metadata for TensorFlow training.
 */
object MetadataGenerator {

  private val fileSystem = FileSystem.get(new JobConf())

  /**
   * Get feature and label (optional) columns in array buffers from the metadata.
   *
   * @param metadataJson A metadata Json String
   * @return Feature and label columns.
   */
  def getFeatureAndLabelColumns(metadataJson: String):
    (mutable.ArrayBuffer[TensorMetadata], Option[mutable.ArrayBuffer[TensorMetadata]]) = {

    implicit val formats = DefaultFormats + new EnumNameSerializer(DataType)

    // Use JSON4S to parse and extract the metadata.
    val inputMetadata = parse(metadataJson).extract[DatasetMetadata]

    // Parse input metadata
    val featureColumns = mutable.ArrayBuffer[TensorMetadata](inputMetadata.features: _*)
    val labelColumnsOpt = inputMetadata.labels match {
      case Some(inputLabels) => Some(mutable.ArrayBuffer[TensorMetadata](inputLabels: _*))
      case None => None
    }

    (featureColumns, labelColumnsOpt)
  }

  /**
   * Add new columns to the input metadata with the new schema.
   *
   * @param dfSchema Schema of the new data frame with additional columns
   * @param inputMetadataFile Input metadata file
   * @param outputMetadataFile Output metadata file
   * @param dataFormat Data format, e.g. Avro or TFRecord
   */
  def addColumnsToMetadata(
    dfSchema: StructType,
    inputMetadataFile: String,
    outputMetadataFile: String,
    dataFormat: String): Unit = {

    // Read input metadata.
    val metadataJson = IoUtils.readFile(fileSystem, inputMetadataFile)
    val (inputFeatureColumns, inputLabelColumnsOpt) = getFeatureAndLabelColumns(metadataJson)
    val columnMap = buildColumnMap(inputFeatureColumns, inputLabelColumnsOpt)

    // Parsing input dataframe schema, append new columns if exist.
    appendNewColumns(dfSchema, columnMap, inputFeatureColumns, dataFormat)

    val metadata = inputLabelColumnsOpt match {
      case Some(inputLabelColumns) => DatasetMetadata(
        features = inputFeatureColumns,
        labels = Some(inputLabelColumns))
      case None => DatasetMetadata(features = inputFeatureColumns)
    }
    val metadataJsonString = JsonUtils.toJsonString(metadata)
    IoUtils.writeFile(fileSystem, new Path(outputMetadataFile), metadataJsonString)
  }

  /**
   * Create metadata for each partition. The metadata is used for random effect training.
   *
   * @param dfSchema Input dataframe schema, which is generated by joining the scores
   *                 from previous stage and features from current stage.
   * @param inputMetadataFile Input metadata used for random effect data processing.
   * @param outputMetadataFile Output metadata file that augmented with new columns.
   * @param dataFormat The format of the corresponding dataset, either avro or tfrecord. This
   *                   is needed to decide how sparse features are represented. For avro data
   *                   format, the indices and values are sub-fields of the feature column.
   *                   For tfrecord, the indices and values are two separate columns
   */
  def saveMetaDataForPartitions(
    dfSchema: StructType,
    inputMetadataFile: String,
    outputMetadataFile: String,
    dataFormat: String): Unit = {

    // Read input metadata.
    val metadataJson = IoUtils.readFile(fileSystem, inputMetadataFile)
    val (inputFeatureColumns, inputLabelColumnsOpt) = getFeatureAndLabelColumns(metadataJson)
    val columnMap = buildColumnMap(inputFeatureColumns, inputLabelColumnsOpt)

    // Parsing input dataframe schema, append new columns if it does not exist already.
    appendNewColumns(dfSchema, columnMap, inputFeatureColumns, dataFormat)

    // Save the modified metadata
    val metadata = inputLabelColumnsOpt match {
      case Some(inputLabelColumns) => DatasetMetadata(
        features = inputFeatureColumns,
        labels = Some(inputLabelColumns))
      case None => DatasetMetadata(features = inputFeatureColumns)
    }
    val metadataJsonString = JsonUtils.toJsonString(metadata)
    IoUtils.writeFile(fileSystem, new Path(outputMetadataFile), metadataJsonString)
  }

  /**
   * Build a map column name -> tensor metadata
   *
   * @param inputFeatureColumns Input feature columns
   * @param inputLabelColumnsOpt An option of input label columns
   * @return The constructed map
   */
  private[data] def buildColumnMap(
    inputFeatureColumns: Seq[TensorMetadata],
    inputLabelColumnsOpt: Option[Seq[TensorMetadata]] = None): Map[String, TensorMetadata] = {
    val features = inputFeatureColumns.map(f => f.name -> f)
    inputLabelColumnsOpt match {
      case Some(inputLabelColumns) =>
        val labels = inputLabelColumns.map(l => l.name -> l)
        (features ++ labels).toMap
      case None => features.toMap
    }
  }

  /**
   * Check if a column contains only simple numeric values, instead of arrays or structs.
   *
   * @param field The column struct field
   * @return The boolean result
   */
  private[data] def isSimpleColumn(
    field: StructField
  ): Boolean = {
    field.dataType.isInstanceOf[NumericType]
  }

  /**
   * Check if a column contains only Array of simple numeric values, instead of arrays of structs.
   *
   * @param field The column struct field
   * @return The boolean result
   */
  private[data] def isSimpleArrayTypeColumn(
    field: StructField
  ): Boolean = {
    field.dataType.isInstanceOf[ArrayType] && field.dataType.asInstanceOf[
      ArrayType].elementType.isInstanceOf[NumericType]
  }

  /**
   * Append new columns from the dataframe to the metadata, for simple numerical type or simple array type,
   * the shape is given by '[]'. Other types are not supported.
   *
   * @param dfSchema Input dataframe schema
   * @param columnMap Existing metadata in form of a map
   * @param inputFeatureColumns The mutable arraybuffer where new
   *                            column metadata is appended
   * @param dataFormat Avro or tfrecord
   */
  private[data] def appendNewColumns(
    dfSchema: org.apache.spark.sql.types.StructType,
    columnMap: Map[String, TensorMetadata],
    inputFeatureColumns: mutable.ArrayBuffer[TensorMetadata],
    dataFormat: String): Unit = {
    dfSchema.foreach { field =>
      if (!(columnMap.contains(field.name)
        || (dataFormat == TFRECORD
        && isSparseColumnComponent(columnMap, field.name)))) {
        if (isSimpleColumn(field)) {
          val newFeature = TensorMetadata(
            field.name,
            mapSparkToConfigDataType(field.dataType),
            Seq(),
            false)
          inputFeatureColumns += newFeature
        } else if (isSimpleArrayTypeColumn(field)) {
          val newFeature = TensorMetadata(
            field.name,
            mapSparkToConfigDataType(field.dataType.asInstanceOf[ArrayType].elementType),
            Seq(), // the array may be of variable length, use "[]" for the shape.
            false)
          inputFeatureColumns += newFeature
        } else {
          throw new IllegalArgumentException(s"Can not handle complex column ${field.name}")
        }
      }
    }
  }

  /**
   * Check if the column is one of the sparse column components.
   * Feature convertor splits a name-term-value column to two columns
   * if TFRecord is used. i.e. global -> [global_indices, global_values]
   * The metadata only keeps the root "global", we don't insert the
   * indices and values columns. Keep it under the hood instead.
   * So a column name such as "global_values" is a component of "global" if
   * it ends with either "_values" or "_indices",
   * and its root, e.g. "global" is a column name in the metadata,
   * and the root column is sparse in the metadata.
   *
   * @param columnMap A map of all the columns from the metadata.
   * @param columnName The column name to be checked.
   * @return Boolean result whether the column is a component or not.
   */
  private[data] def isSparseColumnComponent(
    columnMap: Map[String, TensorMetadata],
    columnName: String
  ): Boolean = {

    val pattern = s"(.*)_(${INDICES}|${VALUES})".r
    val root = columnName match {
      case pattern(root, suffix) => root
      case _ => null
    }
    (root != null) && (columnMap.contains(root)) && columnMap(root).isSparse
  }
}
